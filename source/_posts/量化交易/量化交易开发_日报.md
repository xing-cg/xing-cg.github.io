---
title: 量化交易开发_日报
categories:
  - - 量化交易
tags:
date: 2025/9/9
updated: 2025/9/15
comments:
published: false
---
# day001 - 20250908
基于Linux Mint 22配置环境。
## 中文输入法
先左下角搜索 input method
左边栏选择简体中文。
上边的输入法框架选择 Fcitx。
按照提示步骤进行配置：
1. 先装语言支持包
2. 再切换输入法框架为 fcitx
3. 再重登账户
4. 左下角搜索 fcitx configure
    1. 在其中搜索添加 Pinyin 输入法，搜索时取消勾选 Only Show Language
    2. 尽量把英文输入法设为默认第一个，中文在第二个。
5. 这样，Ctrl + Space 就可以切换输入法了。同时，在使用中文输入法时，shift 可以切换中英文。
## Clash 命令行
拷了一份 `/bin/clash（目录）`到 home。
目录下有：
```
cache.db clash-linux-amd64-v1.16.0 config.yaml Country.mmdb
```
运行的命令：
```sh
./clash-linux-amd64-v1.16.0 -d .
```
之后：
```
INFO[0000] RESTful API listening at: 127.0.0.1:9090     
INFO[0000] HTTP proxy listening at: 127.0.0.1:7890      
INFO[0000] SOCKS proxy listening at: 127.0.0.1:7891 
```
在终端环境下，需要配置proxy：
```bash
cxing@qiyan-ThinkBook-14-G8-IRL:~$ cat proxy.bashrc 
export no_proxy=localhost,127.0.0.0/8,::1
export https_proxy=http://127.0.0.1:7890/
export http_proxy=http://127.0.0.1:7890/
export all_proxy=socks://127.0.0.1:7891/
```
要加载配置，需要`source proxy.bashrc`。

但是ping、ssh默认不走http，所以`ping`、`git clone git@github.com:xxx/xxx.git`依旧会超时。
可以编辑`vim ~/.ssh/config`
```config
Host github.com
  HostName github.com
  User git
  ProxyCommand nc -x 127.0.0.1:7891 %h %p
```
这样，git就可以走代理端口了。
## VS Code配置
配置`~/.ssh/config`

```
Host 192.168.2.151
  HostName 192.168.2.151
  Port 10022
  User cxing
```
## 数据处理开发
见独立文章。
## deb包安装器卡住
遇到的问题是：在Linux Mint 22上安装cursor的deb包时，一直卡在“waiting for mint-refresh-ca to exit”。

可能的原因是：在安装过程中，系统会触发更新证书存储的操作（mint-refresh-ca），通常是因为系统证书更新进程冲突或阻塞。

可以尝试:
1. 终止阻塞的进程：`sudo killall mint-refresh-ca`。如果提示无此进程，直接进入下一步。
2. 手动更新证书：`sudo mint-refresh-ca`。观察输出是否有错误（如网络问题）。完成后重新安装包。
3. 强制完成安装（终极解决）

```bash
sudo dpkg --configure -a  # 修复未完成的安装
sudo apt install -f       # 修复依赖
```

4. 手动单独安装

```bash
sudo dpkg -i xxxxxx.deb           # 替换为实际路径
sudo apt install -f               # 自动修复依赖
```

# day002 - 20250909


上午，重新理解 3 个数据文件中每个字段的意义，以及行数据呈现的特征。

下午，确认项目整体的数据结构。

为了避免每次都从超大的`futopt.csv`中提取某一类期权。可以先用 Python 进行筛选。单独筛选出filteredMO2509.csv。

之后，就可以按照公式，计算同一时间戳，filteredMO2509 中数据 和 IM2509 中数据 的差异了。

重点是要对齐时间戳！不然数据会错乱。

第二是要加日志。这样可以帮助核对每一次计算时，时间戳是对齐的。也可以观察有异常数据时，期权信息的特征。
## 解压xz文件
| 方法类型                | 命令示例                                           | 适用场景                        | 备注                     |
| ------------------- | ---------------------------------------------- | --------------------------- | ---------------------- |
| ​**​解压单一.xz文件​**​   | `xz -d filename.xz`                            | 解压单个.xz文件，​**​默认删除​**​原压缩文件 |                        |
|                     | `unxz filename.xz`                             | 同上，`unxz`是 `xz -d`的别名       |                        |
|                     | `xz -k -d filename.xz`或 `unxz -k filename.xz`  | 解压单个.xz文件，​**​保留​**​原压缩文件   | `-k`/`--keep`选项用于保留原文件 |
| ​**​解压.tar.xz归档​**​ | `tar -Jxf archive.tar.xz`                      | 直接解压.tar.xz文件到当前目录          | `-J`选项专门处理xz压缩         |
|                     | `tar -Jxf archive.tar.xz -C /target/directory` | 直接解压.tar.xz文件到​**​指定目录​**​  | `-C`选项后接目标目录路径         |
| ​**​仅查看内容​**​       | `xzcat filename.xz`                            | 查看.xz压缩文件的文本内容，不解压          |                        |
|                     | `xz -l filename.xz`                            | 列出.xz文件的压缩信息（如压缩率、大小）       |                        |

## g++15的配置
如何测试服务器上编译器是否支持C++23。
```bash
g++ --std=c++23 -dM -E -x c++ /dev/null | grep __cplusplus
```

如果输出显示 `__cplusplus`的值为 `202302L`或更高，则编译器支持C++23。

但实际输出的是2021。
```cpp
#define __cplusplus 202100L
```


可以 `source turing001:/tmp ` 下的gcc配置文件（`gcc14.bashrc`、`gcc15.bashrc`），调整编译器版本。


# day003 - 20250910
上午，学习Google Cpp代码规范。
下午，总结Google Cpp代码规范。学习`C++23`标准库引入的`flat_map`结构。了解`Abseil::flat_hash_map`和Boost库的`unordered_flat_map`结构。对比、总结：设计理念、差异、适用场景。
## Linux内核相关
Linux insides
## 反汇编工具
https://godbolt.org/



# day004 - 20250911
继续Day1、2的数据处理开发。截止day4上午，目前的代码只是能简单地处理IM和futopt中MO的关系。要做到给出一个20250905.0.csv中的某个Ticker Name对应的“期货合约”时间序列表（结构和`IM2509.csv`一样的），

> IM2509 表和 futopt 表的区别就在于，前者是期货，后者是期权。前者即期货没有C/P 类型，也没有行权价。

## 正则表达式的应用
```
MO2509-[PC]-[0-9]{1,}\.[A-Z]{1,}
```

在`20250905.0.csv`中搜索出
1. `MO2025-`开头的，
2. 后面跟了一个P或者C。即看涨看跌类型
3. 之后再连接一个`-`
4. 之后跟大于等于1个的数字，即行权价
5. 之后跟一个`.`
6. 之后跟大于等于1个的大写字母。表示交易所代号

搜索出来 MO2509 的 C、P 各有 43 个，共有 86 个。

搜索出来这些有什么用呢？

这可以确定 `futopt.csv` 中 期权代码 （比如MO2509-C-4000.CFFEX）的范围。

从而，去计算。

## okm_ADA-USDT-SWAP.csv和oks_ADA-USDT-SWAP.csv的对比
okm_ADA-USDT-SWAP.csv：
```
ets,lts,price,volume,side,tid
1757289600009,1757289600013776,0.8348,1270,s,283497115
1757289600009,1757289600013788,0.8348,20,s,283497116
1757289600009,1757289600013794,0.8348,10,s,283497117
1757289600011,1757289600016126,0.8348,20,s,283497118
1757289600118,1757289600122803,0.8347,20,s,283497119
1757289600138,1757289600142341,0.8346,20,s,283497120
1757289600139,1757289600142344,0.8346,10,s,283497121
1757289600336,1757289600341498,0.8346,20,s,283497122
1757289600338,1757289600341509,0.8346,20,s,283497123
1757289600343,1757289600347727,0.8347,30,b,283497124
1757289600375,1757289600379370,0.8346,20,s,283497125
1757289600538,1757289600541754,0.8346,10,s,283497126
1757289600946,1757289600951014,0.8347,220.00000000000003,b,283497127
```

oks_ADA-USDT-SWAP.csv：
```
ets,lts,price,volume,side,tid
1757289600009,1757289600014968,0.8348,1270,s,283497115
1757289600009,1757289600014972,0.8348,20,s,283497116
1757289600009,1757289600014974,0.8348,10,s,283497117
1757289600011,1757289600017844,0.8348,20,s,283497118
1757289600118,1757289600125164,0.8347,20,s,283497119
1757289600138,1757289600144184,0.8346,20,s,283497120
1757289600139,1757289600144188,0.8346,10,s,283497121
1757289600336,1757289600343125,0.8346,20,s,283497122
1757289600338,1757289600343131,0.8346,20,s,283497123
1757289600343,1757289600349304,0.8347,30,b,283497124
1757289600375,1757289600380921,0.8346,20,s,283497125
1757289600538,1757289600543418,0.8346,10,s,283497126
1757289600946,1757289600952459,0.8347,220.00000000000003,b,283497127
```

通常数据对比：
1. 列出差异时间段；
2. 相同id对比数据差异

两个CSV文件 (`oks_ADA-USDT-SWAP.csv`和 `okm_ADA_USDT_SWAP.csv`) 包含了加密货币ADA与USDT永续合约的交易数据，这些数据通常来自交易所（例如OKX）。两个文件的结构一致，主要区别在于时间戳 (`ets`和 `lts`) 的具体数值有些许差异，这可能源于数据采集的频率、来源或处理方式的细微不同。

下面是各个字段的含义解析：

| 字段名            | 含义                                                                          | 示例                 |
| -------------- | --------------------------------------------------------------------------- | ------------------ |
| ​**​ets​**​    | 事件时间戳 (Event Timestamp)。交易​**​发生​**​的时间，通常以毫秒为单位表示Unix时间戳。                  | `1757289600009`    |
| ​**​lts​**​    | 本地时间戳 (Local Timestamp)。数据被​**​记录​**​或​**​接收​**​到本地系统的时间，通常也以毫秒为单位的Unix时间戳。 | `1757289600014968` |
| ​**​price​**​  | 交易成交的价格                                                                     | `0.8348`           |
| ​**​volume​**​ | 交易成交的数量                                                                     | `1270`             |
| ​**​side​**​   | 交易方向。通常 `b`代表"buy"（买方主动成交），`s`代表"sell"（卖方主动成交）。                             | `s`                |
| ​**​tid​**​    | 交易ID (Trade ID)。交易所为每笔交易分配的唯一标识符。                                           | `283497115`        |
文件差异说明：
两个文件记录了​​相同的交易​​（交易ID tid完全一致），但 ets和 lts时间戳存在细微差别。这通常是由于：
1. 数据来源或接口不同​​："oks" 和 "okm" 可能代表了不同的数据频道或产品线（例如，一个可能是标准现货数据，另一个可能是衍生品或汇总数据）。
2. 时间戳的精度或来源不同​​：ets和 lts可能分别来自交易所的服务器时间和本地接收时间，不同接口对时间的记录方式可能有微小偏差。

这类CSV文件通常用于：
1. 交易记录存档​​
2. 量化交易分析​​：例如计算滑点、分析市场微观结构、回测交易策略等。
3. 数据核对​​：比较不同数据源之间的一致性。

进行数据分析时，常用的工具包括​​文本编辑器​​、​​Excel​​等电子表格软件，或​​Python（Pandas库）​​、R等编程语言。

如果打算进行深入的分析，可能会涉及数据清洗（如处理缺失值、异常值）、转换（如将时间戳转换为可读日期）等步骤。
## 基于Python的文件数据差异对比
1. 解压数据文件包
2. 主要对比的差异
    1. ets 缺失时间段
    2. 相同 tid 对比字段差异
### 解压
```python
def check_and_extract(file_path, extract_to=None):
    """
    检查文件是否为压缩包，如果是则解压，返回待读取的实际文件路径。
    
    参数:
    file_path (str): 待检查的输入文件路径
    extract_to (str, optional): 指定解压目录，默认为包含输入文件的目录
    
    返回:
    str: 要读取的文件路径（如果解压，则返回解压后的文件路径；否则返回原始路径）
    """
    if extract_to is None:
        extract_to = os.path.dirname(file_path) # 默认解压到输入文件所在目录

    # 获取文件扩展名以便判断类型
    file_ext = os.path.splitext(file_path)[1].lower()
    # 处理 .xz 文件
    if file_ext == '.xz':
        try:
            # 假设压缩包内只有一个文件，且我们去掉了 .xz 后缀作为解压后的文件名
            output_file_path = os.path.join(extract_to, os.path.basename(file_path)[:-3])
            with lzma.open(file_path, 'rb') as f_in:
                with open(output_file_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            print(f"已解压 .xz 文件: {os.path.basename(file_path)} -> {os.path.basename(output_file_path)}")
            return output_file_path # 返回解压后的文件路径
        except Exception as e:
            print(f"解压 .xz 文件 {file_path} 时出错: {e}")
            return file_path # 如果解压失败，返回原始路径
    # 如果不是支持的压缩格式，则直接返回原文件路径
    else:
        print(f"文件 {os.path.basename(file_path)} 不是压缩包或者不是支持的压缩格式，直接读取。")
        return file_path
```
### 对比缺失时间点
用到的时间函数：
1. `df1['ets_datetime'] = pd.to_datetime(df1['ets'], unit='ms')`。把 csv 文件的 ets 这一列数据全部转为日期类型，存到 df1 的 `ets_datetime` 这一列中

之后：`df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()`

```python
missing_in_df2_vs_df1_points = df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()
```
找出在一个表中有的，但是在另一个表中没有的：
isin函数会检查`df_1`的`ets_datetime`列中的每一个时间戳，判断它是否出现在`df_2`的`ets_datetime`列中。最终生成一个布尔型的Series序列，这个序列的元素数是`df_1`的行数。True表示该时间戳在`df_2`中也有。

Python中的布尔取反是使用 `"~"` 逻辑非操作符。这会让上一步得到的布尔序列全部取反。
最终达到的效果：is not in。即`df_1`的 这一行 ets 在 `df_2` 中没出现。

得到这个布尔Series之后，外面套一个`df_1`，就使用到了布尔索引，从`df_1`这个DataFrame中选取所有对应值为`True`的行。即：筛选出了在df2中找不到的的行。


df2缺失的记录，在df1中是存在的。取出所有在df2中缺失的df1的`ets_datetime`组成一列。然后`unique()`去重。最后，交给`missing_in_df2_vs_df1_points`保管。这是离散的时间点序列。这些数据来源于df1。


然后，`find_missing_periods`就处理这个df2缺失的记录，把离散的合并为连续的。


这个函数的工作：
1. 对时间序列排序，确保是有序的。
2. for循环，对时间序列循环计算间隔，如果间隔在容差范围内，更新`current_end`，如果超过了一定的容差，则记录当前的时间间隔、缺失的tid。添加到`periods`中。
3. 最终，返回给外部的`missing_periods_in_df1`、`missing_periods_in_df2`

```python
def find_missing_periods(missing_timestamps, df_source, tolerance='1min'):
    """
    将离散的缺失时间点列表，合并并标记为连续的时间段。
    同时从源DataFrame中获取这些时间段附近的数据，用于上下文参考。

    参数:
    missing_timestamps (pd.Series): 缺失的时间点列表。
    df_source (pd.DataFrame): 源DataFrame，用于获取时间戳对应的其他数据（如tid）。
    tolerance (str): 用于合并时间点的频率容差（例如，'1min', '1s'），默认为'1min'。

    返回:
    list: 一个列表，每个元素是一个字典，表示一个缺失时间段，包含开始时间、结束时间、持续时长和该时间段内的示例TID。
    """
    if len(missing_timestamps) == 0:
        return []

    # 确保时间戳是排序的
    missing_sorted = pd.Series(missing_timestamps).sort_values().reset_index(drop=True)
    
    periods = []
    current_start = missing_sorted.iloc[0]
    current_end = current_start
    # 用于记录当前时间段内的一个示例TID（从源数据中获取）
    example_tid = df_source[df_source['ets_datetime'] == current_start]['tid'].iloc[0] if not df_source[df_source['ets_datetime'] == current_start].empty else None

    for i in range(1, len(missing_sorted)):
        # 如果当前时间点与下一个时间点之差在容差范围内，则合并到当前时间段
        if missing_sorted.iloc[i] - current_end <= pd.Timedelta(tolerance):
            current_end = missing_sorted.iloc[i]
        else:
            # 当前时间段结束，记录它
            duration = current_end - current_start
            periods.append({
                'start': current_start,
                'end': current_end,
                'duration': duration,
                'example_tid': example_tid # 记录这个时间段的一个代表性TID
            })
            # 开始一个新的时间段
            current_start = missing_sorted.iloc[i]
            current_end = current_start
            example_tid = df_source[df_source['ets_datetime'] == current_start]['tid'].iloc[0] if not df_source[df_source['ets_datetime'] == current_start].empty else None

    # 添加最后一个时间段
    duration = current_end - current_start
    periods.append({
        'start': current_start,
        'end': current_end,
        'duration': duration,
        'example_tid': example_tid
    })

    return periods
```


之后，这个`missing_periods_in_df1`就记录了所有df1缺失的时间段；`missing_periods_in_df2`记录了所有df2缺失的时间段。

可以一一打印出来。

```python
def compare_csv_files_ets_duration(df1, df2):
    df1['ets_datetime'] = pd.to_datetime(df1['ets'], unit='ms')  # 如果ets是秒，则unit='s'
    df2['ets_datetime'] = pd.to_datetime(df2['ets'], unit='ms')  # 如果ets是秒，则unit='s'
    # 确定共同的时间范围（起始时间和结束时间）
    start_time = max(df1['ets_datetime'].min(), df2['ets_datetime'].min())
    end_time = min(df1['ets_datetime'].max(), df2['ets_datetime'].max())
    print(f"okm 的时间范围: {df1['ets_datetime'].min()} 至 {df1['ets_datetime'].max()}")
    print(f"oks 的时间范围: {df2['ets_datetime'].min()} 至 {df2['ets_datetime'].max()}")
    print(f"共同的时间范围: {start_time} 至 {end_time}")
    # 找出在df2的时间点中存在，但df1中缺失的时间点
    missing_in_df1_vs_df2_points = df2[~df2['ets_datetime'].isin(df1['ets_datetime'])]['ets_datetime'].unique()
    # 找出在df1的时间点中存在，但df2中缺失的时间点
    missing_in_df2_vs_df1_points = df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()
    print(f"文件2中有而文件1中缺失的ets时间点: {len(missing_in_df1_vs_df2_points)}个")
    print(pd.Series(missing_in_df1_vs_df2_points))
    print(f"文件1中有而文件2中缺失的ets时间点: {len(missing_in_df2_vs_df1_points)}个")
    print(pd.Series(missing_in_df2_vs_df1_points))

    # 调用函数，将缺失点合并为时间段
    # 参数 `df_source` 传入包含缺失点的那个DataFrame，用于获取示例TID
    missing_periods_in_df1 = find_missing_periods(missing_in_df1_vs_df2_points, df2) # df2中有但df1缺失，所以源数据是df2
    missing_periods_in_df2 = find_missing_periods(missing_in_df2_vs_df1_points, df1) # df1中有但df2缺失，所以源数据是df1
    print("\n" + "="*50)
    print("缺失时间段分析报告")
    print("="*50)

    # 输出文件1中的缺失时间段（即df2有，df1没有的）
    print(f"\n在文件 2 oks 中存在，但在文件 1 okm 中缺失的时间段 (共{len(missing_periods_in_df1)}段):")
    if missing_periods_in_df1:
        for i, period in enumerate(missing_periods_in_df1, 1):
            print(f"  段{i}: {period['start']} 至 {period['end']} (持续: {period['duration']}) | 示例TID: {period['example_tid']}")
    else:
        print("  (无)")

    # 输出文件2中的缺失时间段（即df1有，df2没有的）
    print(f"\n在文件 1 okm 中存在，但在文件 2 oks 中缺失的时间段 (共{len(missing_periods_in_df2)}段):")
    if missing_periods_in_df2:
        for i, period in enumerate(missing_periods_in_df2, 1):
            print(f"  段{i}: {period['start']} 至 {period['end']} (持续: {period['duration']}) | 示例TID: {period['example_tid']}")
    else:
        print("  (无)")
```

### 如何通用化程序
```cpp
        # 比较逻辑
        # 合并两个数据集以便比较
        merged_df = pd.merge(df1, df2, on='tid', suffixes=('_okm', '_oks'))

        # 计算时间戳差异
        merged_df['ets_diff_ms'] = (merged_df['ets_okm'] - merged_df['ets_oks'])
        merged_df['lts_diff_us'] = (merged_df['lts_okm'] - merged_df['lts_oks'])

        # 计算其他字段差异
        merged_df['price_diff'] = merged_df['price_okm'] - merged_df['price_oks']
        merged_df['volume_diff'] = merged_df['volume_okm'] - merged_df['volume_oks']
        merged_df['side_diff'] = merged_df['side_okm'] != merged_df['side_oks']
```
程序中写死了列名称前缀比如`okm`、`oks`。这是对通用性很不友好的。

尽量从文件名解析出来第一个下划线前的标识符，比如`okm_ADA-USDT-SWAP.csv`，的标识符就是okm。

```cpp
def extract_and_remove_slash(original_string):
    """
    1. 首先提取第一个下划线 '_' 之前的内容。
    2. 然后在第一步的结果基础上，去除最后一个斜杠 '/' 及其之前的内容（如果存在的话），只保留最后一个斜杠之后的部分。
    3. 最后在最前面加个'_'
    参数:
        original_string (str): 待处理的原始字符串。
        
    返回:
        str: 最终处理后的字符串。

    效果:
        原字符串: 'path/to/my_file.txt' -> 最终结果: '_my'
    """
    
    # 第一步：提取第一个下划线 '_' 之前的内容
    # 使用 partition() 方法，它总是返回一个三元组 (part_before, separator, part_after)
    part_before_underscore, _, _ = original_string.partition('_')
    
    # 第二步：在第一步的结果上，去除最后一个斜杠 '/' 及其之前的内容
    # 使用 rsplit() 并指定最大分割次数为1，然后取最后一部分
    final_result = part_before_underscore.rsplit('/', 1)[-1]
    
    return "_" + final_result
```


# day005 - 20250912
## Zhao 讲解总体业务流程
根据 frequency，每隔一分钟，做一个 predict（asset，security），return 一个 30 min、90 min、360 min、1440 min 的 edge ，比如 预测到 AU（Gold）`+1%`，那就做一个 optimization，比如 Current Position 是 1 shares，那么 target position 是 3 shares，delta 就是 `+2` shares。

Ask，Bid。
$ 100，$ 99。

Taker，Maker。
$ 200，$ 198。
## 确认 checkAlerts 逻辑
要求持续一段时间超过阈值才报警。

```cpp
// 检查最近duration个数据点
for (auto it = series.rbegin(); it != series.rend() && it != series.rbegin() + duration; ++it)
{
    if (std::abs(it->first) > threshold)
    {
        pdiffAlertCount++;
    }
    if (std::abs(it->second) > threshold)
    {
        ndiffAlertCount++;
    }
}
```

什么是`series`？
是 MarketDataStore 的 存储 套利机会时间序列 的 `unordered_map<int, std::deque<std::pair<double, double>>> arbitrageSeries`。即映射关系为：`strike -> deque<ArbitrageOpportunity>`

也就是说，根据不同的 strike，存储了多个双端队列，双端队列存放的是`一对儿double`。这一对儿 double 对应的就是计算出的对应一个 strike
 的 `pdiff, ndiff`。

这一对儿 值 什么时候 插入？

在 test.cpp 中，processData 时，对 fut 文件 的 每一个 时间戳 遍历 处理：

对齐 每个 fut 和 opt 的时间戳。
之后，计算 每一对儿时间戳 对齐后 的套利机会（calculateAlignedArbitrage）

## MarketDataStore 数据结构

某一期货的交易记录（IM2509），和 所有 期权的 交易记录（optfut 或 经筛选的`filtered_MO2509`），都是按行有序的。

如果只是研究死文件，即只会push back，那就用vector。
如果要流式分析，会涉及到pop front，push back，需要用到 deque。

# day006 - 20250915
## 改进文件数据差异对比程序
见[基于Python的文件数据差异对比](#基于Python的文件数据差异对比)。

1. 为了避免最后打印过多的 lts diff 信息，导致 log 文件超大，设置了verbose开关。
2. 由于目前写的Python程序都是print，默认打印到标准输出。为了简单，直接用`sh`程序语言包装。在内部封装重定向输出到日志文件的操作。
3. 分析的文件名不再默认带有`okm_`、`oks_`前缀。而是以目录为分辨依据。需要改变提取前缀的逻辑——`extract_prefix_from_path`。

ToDo：
脚本程序最后的
```python
# 保存差异结果到CSV文件

significant_etime_diff.to_csv('ets_differences.csv', index=False)

significant_ltime_diff.to_csv('lts_differences.csv', index=False)

if not data_differences.empty:

data_differences.to_csv('data_differences.csv', index=False)

print("\n差异分析已保存到 ets/lts_differences.csv 和 data_differences.csv")
```
这个名字不能写死，不然新运行一遍之后之前的文件内容就会被覆写。
需要修改逻辑：
方案1. 每个`ets/lts_differences.csv`和`data_differences.csv`都带后缀。
方案2. 按日期、对比的币种类型创建文件夹，把日志文件分类存放。比如：20250911_BTC-USDT-SWAP就应该放到：`okm_oks_diff/20250911/BTC-USDT-SWAP/`下。
## 根据`differ_on_tid`提取出的缺失的时间段，进行日志分析
分析了OKS、OKM，2个服务器在20250911、20250912的日志文件。

两类：BTC-USDT-SWAP、ETH-USDT-SWAP。
1. 先做一个整体分析：BTC的缺失时间段分析报告、ETH的缺失时间段分析报告。
    1. 两者如果是基本上吻合的，大概率是系统服务器不工作，或者断网了。
        1. 这种情况只基于BTC-USDT-SWAP的时间点分析一次就可以了
2. 具体分析：按照有问题的时间点，结合dump出的`okm_okx.log`、`oks_okx.log`，去其中检索该时间点的上下文信息，寻找Warning或Error信息。
3. 统计错误原因。
## 字体安装
参考：https://www.cnblogs.com/xia-weiwen/p/10336896.html

在Linux下做文档、作图的时候，可能需要用到Arial和Times New Roman等字体。但是由于版权问题，Linux一般是不直接提供这些字体的。

注意字体也是有版权的！不过有版权也不代表一定会收费。

如何安装呢？

以Ubuntu为例，执行以下指令即可：

```bash
$ sudo apt install ttf-mscorefonts-installer # 安装
$ sudo fc-cache # 生效
```

由于前面提到版权问题，虽然这部分字体不收费，但是安装时需要用户同意一些协议，同意即可。

执行完成后，用以下指令确认成功：

```bash
$ fc-match Arial # 查看Arial
$ fc-match Times # 查看Times New Roman
```

或者，如果有字体查看器，可以直接去查看。  
再或者，如果有编辑器之类的软件，可以在选择字体列表里看到新出现的字体。

需要使用这些字体的软件，如果在安装之前就打开了，那么安装完字体后，这些软件需要重启才能看到新安装的字体。