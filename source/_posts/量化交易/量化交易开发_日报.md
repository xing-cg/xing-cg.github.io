---
title: 量化交易开发_日报
categories:
  - - 量化交易
tags:
date: 2025/9/9
updated: 2025/9/15
comments:
published: false
---
# day001 - 20250908
基于Linux Mint 22配置环境。
## 中文输入法
先左下角搜索 input method
左边栏选择简体中文。
上边的输入法框架选择 Fcitx。
按照提示步骤进行配置：
1. 先装语言支持包
2. 再切换输入法框架为 fcitx
3. 再重登账户
4. 左下角搜索 fcitx configure
    1. 在其中搜索添加 Pinyin 输入法，搜索时取消勾选 Only Show Language
    2. 尽量把英文输入法设为默认第一个，中文在第二个。
5. 这样，Ctrl + Space 就可以切换输入法了。同时，在使用中文输入法时，shift 可以切换中英文。
## Clash 命令行
拷了一份 `/bin/clash（目录）`到 home。
目录下有：
```
cache.db clash-linux-amd64-v1.16.0 config.yaml Country.mmdb
```
运行的命令：
```sh
./clash-linux-amd64-v1.16.0 -d .
```
之后：
```
INFO[0000] RESTful API listening at: 127.0.0.1:9090     
INFO[0000] HTTP proxy listening at: 127.0.0.1:7890      
INFO[0000] SOCKS proxy listening at: 127.0.0.1:7891 
```
在终端环境下，需要配置proxy：
```bash
cxing@qiyan-ThinkBook-14-G8-IRL:~$ cat proxy.bashrc 
export no_proxy=localhost,127.0.0.0/8,::1
export https_proxy=http://127.0.0.1:7890/
export http_proxy=http://127.0.0.1:7890/
export all_proxy=socks://127.0.0.1:7891/
```
要加载配置，需要`source proxy.bashrc`。

但是ping、ssh默认不走http，所以`ping`、`git clone git@github.com:xxx/xxx.git`依旧会超时。
可以编辑`vim ~/.ssh/config`
```config
Host github.com
  HostName github.com
  User git
  ProxyCommand nc -x 127.0.0.1:7891 %h %p
```
这样，git就可以走代理端口了。
## VS Code配置
配置`~/.ssh/config`

```
Host 192.168.2.151
  HostName 192.168.2.151
  Port 10022
  User cxing
```
## 项目：数据处理开发 - pdiff ndiff
见独立文章。
## deb包安装器卡住
遇到的问题是：在Linux Mint 22上安装cursor的deb包时，一直卡在“waiting for mint-refresh-ca to exit”。

可能的原因是：在安装过程中，系统会触发更新证书存储的操作（mint-refresh-ca），通常是因为系统证书更新进程冲突或阻塞。

可以尝试:
1. 终止阻塞的进程：`sudo killall mint-refresh-ca`。如果提示无此进程，直接进入下一步。
2. 手动更新证书：`sudo mint-refresh-ca`。观察输出是否有错误（如网络问题）。完成后重新安装包。
3. 强制完成安装（终极解决）

```bash
sudo dpkg --configure -a  # 修复未完成的安装
sudo apt install -f       # 修复依赖
```

4. 手动单独安装

```bash
sudo dpkg -i xxxxxx.deb           # 替换为实际路径
sudo apt install -f               # 自动修复依赖
```

# day002 - 20250909


上午，重新理解 3 个数据文件中每个字段的意义，以及行数据呈现的特征。

下午，确认项目整体的数据结构。

为了避免每次都从超大的`futopt.csv`中提取某一类期权。可以先用 Python 进行筛选。单独筛选出filteredMO2509.csv。

之后，就可以按照公式，计算同一时间戳，filteredMO2509 中数据 和 IM2509 中数据 的差异了。

重点是要对齐时间戳！不然数据会错乱。

第二是要加日志。这样可以帮助核对每一次计算时，时间戳是对齐的。也可以观察有异常数据时，期权信息的特征。
## 解压xz文件
| 方法类型                | 命令示例                                           | 适用场景                        | 备注                     |
| ------------------- | ---------------------------------------------- | --------------------------- | ---------------------- |
| ​**​解压单一.xz文件​**​   | `xz -d filename.xz`                            | 解压单个.xz文件，​**​默认删除​**​原压缩文件 |                        |
|                     | `unxz filename.xz`                             | 同上，`unxz`是 `xz -d`的别名       |                        |
|                     | `xz -k -d filename.xz`或 `unxz -k filename.xz`  | 解压单个.xz文件，​**​保留​**​原压缩文件   | `-k`/`--keep`选项用于保留原文件 |
| ​**​解压.tar.xz归档​**​ | `tar -Jxf archive.tar.xz`                      | 直接解压.tar.xz文件到当前目录          | `-J`选项专门处理xz压缩         |
|                     | `tar -Jxf archive.tar.xz -C /target/directory` | 直接解压.tar.xz文件到​**​指定目录​**​  | `-C`选项后接目标目录路径         |
| ​**​仅查看内容​**​       | `xzcat filename.xz`                            | 查看.xz压缩文件的文本内容，不解压          |                        |
|                     | `xz -l filename.xz`                            | 列出.xz文件的压缩信息（如压缩率、大小）       |                        |

## g++15的配置
如何测试服务器上编译器是否支持C++23。
```bash
g++ --std=c++23 -dM -E -x c++ /dev/null | grep __cplusplus
```

如果输出显示 `__cplusplus`的值为 `202302L`或更高，则编译器支持C++23。

但实际输出的是2021。
```cpp
#define __cplusplus 202100L
```


可以 `source turing001:/tmp ` 下的gcc配置文件（`gcc14.bashrc`、`gcc15.bashrc`），调整编译器版本。


# day003 - 20250910
上午，学习Google Cpp代码规范。
下午，总结Google Cpp代码规范。学习`C++23`标准库引入的`flat_map`结构。了解`Abseil::flat_hash_map`和Boost库的`unordered_flat_map`结构。对比、总结：设计理念、差异、适用场景。
## Linux内核相关
Linux insides
## 反汇编工具
https://godbolt.org/



# day004 - 20250911
继续Day1、2的数据处理开发。截止day4上午，目前的代码只是能简单地处理IM和futopt中MO的关系。要做到给出一个20250905.0.csv中的某个Ticker Name对应的“期货合约”时间序列表（结构和`IM2509.csv`一样的），

> IM2509 表和 futopt 表的区别就在于，前者是期货，后者是期权。前者即期货没有C/P 类型，也没有行权价。

## 正则表达式的应用
```
MO2509-[PC]-[0-9]{1,}\.[A-Z]{1,}
```

在`20250905.0.csv`中搜索出
1. `MO2025-`开头的，
2. 后面跟了一个P或者C。即看涨看跌类型
3. 之后再连接一个`-`
4. 之后跟大于等于1个的数字，即行权价
5. 之后跟一个`.`
6. 之后跟大于等于1个的大写字母。表示交易所代号

搜索出来 MO2509 的 C、P 各有 43 个，共有 86 个。

搜索出来这些有什么用呢？

这可以确定 `futopt.csv` 中 期权代码 （比如MO2509-C-4000.CFFEX）的范围。

从而，去计算。

## okm_ADA-USDT-SWAP.csv和oks_ADA-USDT-SWAP.csv的对比
okm_ADA-USDT-SWAP.csv：
```
ets,lts,price,volume,side,tid
1757289600009,1757289600013776,0.8348,1270,s,283497115
1757289600009,1757289600013788,0.8348,20,s,283497116
1757289600009,1757289600013794,0.8348,10,s,283497117
1757289600011,1757289600016126,0.8348,20,s,283497118
1757289600118,1757289600122803,0.8347,20,s,283497119
1757289600138,1757289600142341,0.8346,20,s,283497120
1757289600139,1757289600142344,0.8346,10,s,283497121
1757289600336,1757289600341498,0.8346,20,s,283497122
1757289600338,1757289600341509,0.8346,20,s,283497123
1757289600343,1757289600347727,0.8347,30,b,283497124
1757289600375,1757289600379370,0.8346,20,s,283497125
1757289600538,1757289600541754,0.8346,10,s,283497126
1757289600946,1757289600951014,0.8347,220.00000000000003,b,283497127
```

oks_ADA-USDT-SWAP.csv：
```
ets,lts,price,volume,side,tid
1757289600009,1757289600014968,0.8348,1270,s,283497115
1757289600009,1757289600014972,0.8348,20,s,283497116
1757289600009,1757289600014974,0.8348,10,s,283497117
1757289600011,1757289600017844,0.8348,20,s,283497118
1757289600118,1757289600125164,0.8347,20,s,283497119
1757289600138,1757289600144184,0.8346,20,s,283497120
1757289600139,1757289600144188,0.8346,10,s,283497121
1757289600336,1757289600343125,0.8346,20,s,283497122
1757289600338,1757289600343131,0.8346,20,s,283497123
1757289600343,1757289600349304,0.8347,30,b,283497124
1757289600375,1757289600380921,0.8346,20,s,283497125
1757289600538,1757289600543418,0.8346,10,s,283497126
1757289600946,1757289600952459,0.8347,220.00000000000003,b,283497127
```

通常数据对比：
1. 列出差异时间段；
2. 相同id对比数据差异

两个CSV文件 (`oks_ADA-USDT-SWAP.csv`和 `okm_ADA_USDT_SWAP.csv`) 包含了加密货币ADA与USDT永续合约的交易数据，这些数据通常来自交易所（例如OKX）。两个文件的结构一致，主要区别在于时间戳 (`ets`和 `lts`) 的具体数值有些许差异，这可能源于数据采集的频率、来源或处理方式的细微不同。

下面是各个字段的含义解析：

| 字段名            | 含义                                                                          | 示例                 |
| -------------- | --------------------------------------------------------------------------- | ------------------ |
| ​**​ets​**​    | 事件时间戳 (Event Timestamp)。交易​**​发生​**​的时间，通常以毫秒为单位表示Unix时间戳。                  | `1757289600009`    |
| ​**​lts​**​    | 本地时间戳 (Local Timestamp)。数据被​**​记录​**​或​**​接收​**​到本地系统的时间，通常也以毫秒为单位的Unix时间戳。 | `1757289600014968` |
| ​**​price​**​  | 交易成交的价格                                                                     | `0.8348`           |
| ​**​volume​**​ | 交易成交的数量                                                                     | `1270`             |
| ​**​side​**​   | 交易方向。通常 `b`代表"buy"（买方主动成交），`s`代表"sell"（卖方主动成交）。                             | `s`                |
| ​**​tid​**​    | 交易ID (Trade ID)。交易所为每笔交易分配的唯一标识符。                                           | `283497115`        |
文件差异说明：
两个文件记录了​​相同的交易​​（交易ID tid完全一致），但 ets和 lts时间戳存在细微差别。这通常是由于：
1. 数据来源或接口不同​​："oks" 和 "okm" 可能代表了不同的数据频道或产品线（例如，一个可能是标准现货数据，另一个可能是衍生品或汇总数据）。
2. 时间戳的精度或来源不同​​：ets和 lts可能分别来自交易所的服务器时间和本地接收时间，不同接口对时间的记录方式可能有微小偏差。

这类CSV文件通常用于：
1. 交易记录存档​​
2. 量化交易分析​​：例如计算滑点、分析市场微观结构、回测交易策略等。
3. 数据核对​​：比较不同数据源之间的一致性。

进行数据分析时，常用的工具包括​​文本编辑器​​、​​Excel​​等电子表格软件，或​​Python（Pandas库）​​、R等编程语言。

如果打算进行深入的分析，可能会涉及数据清洗（如处理缺失值、异常值）、转换（如将时间戳转换为可读日期）等步骤。
## 项目：基于Python的文件数据差异对比
1. 解压数据文件包
2. 主要对比的差异
    1. ets 缺失时间段
    2. 相同 tid 对比字段差异
### 解压
```python
def check_and_extract(file_path, extract_to=None):
    """
    检查文件是否为压缩包，如果是则解压，返回待读取的实际文件路径。
    
    参数:
    file_path (str): 待检查的输入文件路径
    extract_to (str, optional): 指定解压目录，默认为包含输入文件的目录
    
    返回:
    str: 要读取的文件路径（如果解压，则返回解压后的文件路径；否则返回原始路径）
    """
    if extract_to is None:
        extract_to = os.path.dirname(file_path) # 默认解压到输入文件所在目录

    # 获取文件扩展名以便判断类型
    file_ext = os.path.splitext(file_path)[1].lower()
    # 处理 .xz 文件
    if file_ext == '.xz':
        try:
            # 假设压缩包内只有一个文件，且我们去掉了 .xz 后缀作为解压后的文件名
            output_file_path = os.path.join(extract_to, os.path.basename(file_path)[:-3])
            with lzma.open(file_path, 'rb') as f_in:
                with open(output_file_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            print(f"已解压 .xz 文件: {os.path.basename(file_path)} -> {os.path.basename(output_file_path)}")
            return output_file_path # 返回解压后的文件路径
        except Exception as e:
            print(f"解压 .xz 文件 {file_path} 时出错: {e}")
            return file_path # 如果解压失败，返回原始路径
    # 如果不是支持的压缩格式，则直接返回原文件路径
    else:
        print(f"文件 {os.path.basename(file_path)} 不是压缩包或者不是支持的压缩格式，直接读取。")
        return file_path
```
### 对比缺失时间点
用到的时间函数：
1. `df1['ets_datetime'] = pd.to_datetime(df1['ets'], unit='ms')`。把 csv 文件的 ets 这一列数据全部转为日期类型，存到 df1 的 `ets_datetime` 这一列中

之后：`df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()`

```python
missing_in_df2_vs_df1_points = df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()
```
找出在一个表中有的，但是在另一个表中没有的：
isin函数会检查`df_1`的`ets_datetime`列中的每一个时间戳，判断它是否出现在`df_2`的`ets_datetime`列中。最终生成一个布尔型的Series序列，这个序列的元素数是`df_1`的行数。True表示该时间戳在`df_2`中也有。

Python中的布尔取反是使用 `"~"` 逻辑非操作符。这会让上一步得到的布尔序列全部取反。
最终达到的效果：is not in。即`df_1`的 这一行 ets 在 `df_2` 中没出现。

得到这个布尔Series之后，外面套一个`df_1`，就使用到了布尔索引，从`df_1`这个DataFrame中选取所有对应值为`True`的行。即：筛选出了在df2中找不到的的行。


df2缺失的记录，在df1中是存在的。取出所有在df2中缺失的df1的`ets_datetime`组成一列。然后`unique()`去重。最后，交给`missing_in_df2_vs_df1_points`保管。这是离散的时间点序列。这些数据来源于df1。


然后，`find_missing_periods`就处理这个df2缺失的记录，把离散的合并为连续的。


这个函数的工作：
1. 对时间序列排序，确保是有序的。
2. for循环，对时间序列循环计算间隔，如果间隔在容差范围内，更新`current_end`，如果超过了一定的容差，则记录当前的时间间隔、缺失的tid。添加到`periods`中。
3. 最终，返回给外部的`missing_periods_in_df1`、`missing_periods_in_df2`

```python
def find_missing_periods(missing_timestamps, df_source, tolerance='1min'):
    """
    将离散的缺失时间点列表，合并并标记为连续的时间段。
    同时从源DataFrame中获取这些时间段附近的数据，用于上下文参考。

    参数:
    missing_timestamps (pd.Series): 缺失的时间点列表。
    df_source (pd.DataFrame): 源DataFrame，用于获取时间戳对应的其他数据（如tid）。
    tolerance (str): 用于合并时间点的频率容差（例如，'1min', '1s'），默认为'1min'。

    返回:
    list: 一个列表，每个元素是一个字典，表示一个缺失时间段，包含开始时间、结束时间、持续时长和该时间段内的示例TID。
    """
    if len(missing_timestamps) == 0:
        return []

    # 确保时间戳是排序的
    missing_sorted = pd.Series(missing_timestamps).sort_values().reset_index(drop=True)
    
    periods = []
    current_start = missing_sorted.iloc[0]
    current_end = current_start
    # 用于记录当前时间段内的一个示例TID（从源数据中获取）
    example_tid = df_source[df_source['ets_datetime'] == current_start]['tid'].iloc[0] if not df_source[df_source['ets_datetime'] == current_start].empty else None

    for i in range(1, len(missing_sorted)):
        # 如果当前时间点与下一个时间点之差在容差范围内，则合并到当前时间段
        if missing_sorted.iloc[i] - current_end <= pd.Timedelta(tolerance):
            current_end = missing_sorted.iloc[i]
        else:
            # 当前时间段结束，记录它
            duration = current_end - current_start
            periods.append({
                'start': current_start,
                'end': current_end,
                'duration': duration,
                'example_tid': example_tid # 记录这个时间段的一个代表性TID
            })
            # 开始一个新的时间段
            current_start = missing_sorted.iloc[i]
            current_end = current_start
            example_tid = df_source[df_source['ets_datetime'] == current_start]['tid'].iloc[0] if not df_source[df_source['ets_datetime'] == current_start].empty else None

    # 添加最后一个时间段
    duration = current_end - current_start
    periods.append({
        'start': current_start,
        'end': current_end,
        'duration': duration,
        'example_tid': example_tid
    })

    return periods
```


之后，这个`missing_periods_in_df1`就记录了所有df1缺失的时间段；`missing_periods_in_df2`记录了所有df2缺失的时间段。

可以一一打印出来。

```python
def compare_csv_files_ets_duration(df1, df2):
    df1['ets_datetime'] = pd.to_datetime(df1['ets'], unit='ms')  # 如果ets是秒，则unit='s'
    df2['ets_datetime'] = pd.to_datetime(df2['ets'], unit='ms')  # 如果ets是秒，则unit='s'
    # 确定共同的时间范围（起始时间和结束时间）
    start_time = max(df1['ets_datetime'].min(), df2['ets_datetime'].min())
    end_time = min(df1['ets_datetime'].max(), df2['ets_datetime'].max())
    print(f"okm 的时间范围: {df1['ets_datetime'].min()} 至 {df1['ets_datetime'].max()}")
    print(f"oks 的时间范围: {df2['ets_datetime'].min()} 至 {df2['ets_datetime'].max()}")
    print(f"共同的时间范围: {start_time} 至 {end_time}")
    # 找出在df2的时间点中存在，但df1中缺失的时间点
    missing_in_df1_vs_df2_points = df2[~df2['ets_datetime'].isin(df1['ets_datetime'])]['ets_datetime'].unique()
    # 找出在df1的时间点中存在，但df2中缺失的时间点
    missing_in_df2_vs_df1_points = df1[~df1['ets_datetime'].isin(df2['ets_datetime'])]['ets_datetime'].unique()
    print(f"文件2中有而文件1中缺失的ets时间点: {len(missing_in_df1_vs_df2_points)}个")
    print(pd.Series(missing_in_df1_vs_df2_points))
    print(f"文件1中有而文件2中缺失的ets时间点: {len(missing_in_df2_vs_df1_points)}个")
    print(pd.Series(missing_in_df2_vs_df1_points))

    # 调用函数，将缺失点合并为时间段
    # 参数 `df_source` 传入包含缺失点的那个DataFrame，用于获取示例TID
    missing_periods_in_df1 = find_missing_periods(missing_in_df1_vs_df2_points, df2) # df2中有但df1缺失，所以源数据是df2
    missing_periods_in_df2 = find_missing_periods(missing_in_df2_vs_df1_points, df1) # df1中有但df2缺失，所以源数据是df1
    print("\n" + "="*50)
    print("缺失时间段分析报告")
    print("="*50)

    # 输出文件1中的缺失时间段（即df2有，df1没有的）
    print(f"\n在文件 2 oks 中存在，但在文件 1 okm 中缺失的时间段 (共{len(missing_periods_in_df1)}段):")
    if missing_periods_in_df1:
        for i, period in enumerate(missing_periods_in_df1, 1):
            print(f"  段{i}: {period['start']} 至 {period['end']} (持续: {period['duration']}) | 示例TID: {period['example_tid']}")
    else:
        print("  (无)")

    # 输出文件2中的缺失时间段（即df1有，df2没有的）
    print(f"\n在文件 1 okm 中存在，但在文件 2 oks 中缺失的时间段 (共{len(missing_periods_in_df2)}段):")
    if missing_periods_in_df2:
        for i, period in enumerate(missing_periods_in_df2, 1):
            print(f"  段{i}: {period['start']} 至 {period['end']} (持续: {period['duration']}) | 示例TID: {period['example_tid']}")
    else:
        print("  (无)")
```

### 如何通用化程序
```cpp
        # 比较逻辑
        # 合并两个数据集以便比较
        merged_df = pd.merge(df1, df2, on='tid', suffixes=('_okm', '_oks'))

        # 计算时间戳差异
        merged_df['ets_diff_ms'] = (merged_df['ets_okm'] - merged_df['ets_oks'])
        merged_df['lts_diff_us'] = (merged_df['lts_okm'] - merged_df['lts_oks'])

        # 计算其他字段差异
        merged_df['price_diff'] = merged_df['price_okm'] - merged_df['price_oks']
        merged_df['volume_diff'] = merged_df['volume_okm'] - merged_df['volume_oks']
        merged_df['side_diff'] = merged_df['side_okm'] != merged_df['side_oks']
```
程序中写死了列名称前缀比如`okm`、`oks`。这是对通用性很不友好的。

尽量从文件名解析出来第一个下划线前的标识符，比如`okm_ADA-USDT-SWAP.csv`，的标识符就是okm。

```cpp
def extract_and_remove_slash(original_string):
    """
    1. 首先提取第一个下划线 '_' 之前的内容。
    2. 然后在第一步的结果基础上，去除最后一个斜杠 '/' 及其之前的内容（如果存在的话），只保留最后一个斜杠之后的部分。
    3. 最后在最前面加个'_'
    参数:
        original_string (str): 待处理的原始字符串。
        
    返回:
        str: 最终处理后的字符串。

    效果:
        原字符串: 'path/to/my_file.txt' -> 最终结果: '_my'
    """
    
    # 第一步：提取第一个下划线 '_' 之前的内容
    # 使用 partition() 方法，它总是返回一个三元组 (part_before, separator, part_after)
    part_before_underscore, _, _ = original_string.partition('_')
    
    # 第二步：在第一步的结果上，去除最后一个斜杠 '/' 及其之前的内容
    # 使用 rsplit() 并指定最大分割次数为1，然后取最后一部分
    final_result = part_before_underscore.rsplit('/', 1)[-1]
    
    return "_" + final_result
```


# day005 - 20250912
## Zhao 讲解总体业务流程
根据 frequency，每隔一分钟，做一个 predict（asset，security），return 一个 30 min、90 min、360 min、1440 min 的 edge ，比如 预测到 AU（Gold）`+1%`，那就做一个 optimization，比如 Current Position 是 1 shares，那么 target position 是 3 shares，delta 就是 `+2` shares。

Ask，Bid。
$ 100，$ 99。

Taker，Maker。
$ 200，$ 198。
## 确认 checkAlerts 逻辑
要求持续一段时间超过阈值才报警。

```cpp
// 检查最近duration个数据点
for (auto it = series.rbegin(); it != series.rend() && it != series.rbegin() + duration; ++it)
{
    if (std::abs(it->first) > threshold)
    {
        pdiffAlertCount++;
    }
    if (std::abs(it->second) > threshold)
    {
        ndiffAlertCount++;
    }
}
```

什么是`series`？
是 MarketDataStore 的 存储 套利机会时间序列 的 `unordered_map<int, std::deque<std::pair<double, double>>> arbitrageSeries`。即映射关系为：`strike -> deque<ArbitrageOpportunity>`

也就是说，根据不同的 strike，存储了多个双端队列，双端队列存放的是`一对儿double`。这一对儿 double 对应的就是计算出的对应一个 strike
 的 `pdiff, ndiff`。

这一对儿 值 什么时候 插入？

在 test.cpp 中，processData 时，对 fut 文件 的 每一个 时间戳 遍历 处理：

对齐 每个 fut 和 opt 的时间戳。
之后，计算 每一对儿时间戳 对齐后 的套利机会（calculateAlignedArbitrage）

## MarketDataStore 数据结构

某一期货的交易记录（IM2509），和 所有 期权的 交易记录（optfut 或 经筛选的`filtered_MO2509`），都是按行有序的。

如果只是研究死文件，即只会push back，那就用vector。
如果要流式分析，会涉及到pop front，push back，需要用到 deque。

# day006 - 20250915
## 改进文件数据差异对比程序
见[基于Python的文件数据差异对比](#基于Python的文件数据差异对比)。

1. 为了避免最后打印过多的 lts diff 信息，导致 log 文件超大，设置了verbose开关。
2. 由于目前写的Python程序都是print，默认打印到标准输出。为了简单，直接用`sh`程序语言包装。在内部封装重定向输出到日志文件的操作。
3. 分析的文件名不再默认带有`okm_`、`oks_`前缀。而是以目录为分辨依据。需要改变提取前缀的逻辑——`extract_prefix_from_path`。

ToDo：
脚本程序最后的
```python
# 保存差异结果到CSV文件

significant_etime_diff.to_csv('ets_differences.csv', index=False)

significant_ltime_diff.to_csv('lts_differences.csv', index=False)

if not data_differences.empty:

data_differences.to_csv('data_differences.csv', index=False)

print("\n差异分析已保存到 ets/lts_differences.csv 和 data_differences.csv")
```
这个名字不能写死，不然新运行一遍之后之前的文件内容就会被覆写。
需要修改逻辑：
方案1. 每个`ets/lts_differences.csv`和`data_differences.csv`都带后缀。
方案2. 按日期、对比的币种类型创建文件夹，把日志文件分类存放。比如：20250911_BTC-USDT-SWAP就应该放到：`okm_oks_diff/20250911/BTC-USDT-SWAP/`下。
## 根据`differ_on_tid`提取出的缺失的时间段，进行日志分析
分析了OKS、OKM，2个服务器在20250911、20250912的日志文件。

两类：BTC-USDT-SWAP、ETH-USDT-SWAP。
1. 先做一个整体分析：BTC的缺失时间段分析报告、ETH的缺失时间段分析报告。
    1. 两者如果是基本上吻合的，大概率是系统服务器不工作，或者断网了。
        1. 这种情况只基于BTC-USDT-SWAP的时间点分析一次就可以了
2. 具体分析：按照有问题的时间点，结合dump出的`okm_okx.log`、`oks_okx.log`，去其中检索该时间点的上下文信息，寻找Warning或Error信息。
3. 统计错误原因。
## 字体安装
参考：https://www.cnblogs.com/xia-weiwen/p/10336896.html

在Linux下做文档、作图的时候，可能需要用到Arial和Times New Roman等字体。但是由于版权问题，Linux一般是不直接提供这些字体的。

注意字体也是有版权的！不过有版权也不代表一定会收费。

如何安装呢？

以Ubuntu为例，执行以下指令即可：

```bash
$ sudo apt install ttf-mscorefonts-installer # 安装
$ sudo fc-cache # 生效
```

由于前面提到版权问题，虽然这部分字体不收费，但是安装时需要用户同意一些协议，同意即可。

执行完成后，用以下指令确认成功：

```bash
$ fc-match Arial # 查看Arial
$ fc-match Times # 查看Times New Roman
```

或者，如果有字体查看器，可以直接去查看。  
再或者，如果有编辑器之类的软件，可以在选择字体列表里看到新出现的字体。

需要使用这些字体的软件，如果在安装之前就打开了，那么安装完字体后，这些软件需要重启才能看到新安装的字体。

# day007 - 20250916
开始

1. 2 个 ets 之间，500 ms。计算 ask 、bid 的差值。
2. 价格是 0.2 的整数倍。
3. 设置开关，去除 上午 9:29 、 下午 1:00 的极端值。

## depth1的对比

```
ets,lts,cvolume,camount,last,askprc,askvol,bidprc,bidvol,idxprc,markprc,oi
1757567899557,
1757567899562245,
22464.504297815216,
2512269936.7059517,
114046.5,
114046.6,
18.546399585455656,
114046.5,
0.14999999664723873,
nan,
114048.4,
2814289.6400000094
```
1. ets: long: 1757567899557
2. lts: long: 1757567899562245
3. cvolume: double: 22464.504297815216
4. camount: 2512269936.7059517
5. last: 114046.5
6. askprc: 114046.6
7. askvol: 18.546399585455656
8. bidprc: 114046.5
9. bidvol: 0.14999999664723873
10. idxprc: nan
11. markprc: 114048.4
12. oi: 2814289.6400000094

oks/depth1/BTC-USDT-SWAP.csv.xz
```
ets,lts,cvolume,camount,last,askprc,askvol,bidprc,bidvol,idxprc,markprc,oi
1757567899557,1757567899562245,22464.504297815216,2512269936.7059517,114046.5,114046.6,18.546399585455656,114046.5,0.14999999664723873,nan,114048.4,2814289.6400000094
1757567899737,1757567899743118,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.1498999966494739,nan,114048.4,2814289.6400000094
1757567899917,1757567899922300,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.10609999762848019,nan,114048.3,2814289.6400000094
1757567899927,1757567899932401,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.14969999665394426,nan,114048.3,2814289.6400000094
1757567899937,1757567899942392,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.1498999966494739,nan,114048.3,2814289.6400000094
1757567900087,1757567900092491,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.10609999762848019,nan,114048.3,2814289.6400000094
1757567900127,1757567900132289,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.546399585455656,114046.5,0.10599999763071537,nan,114048.2,2814289.6400000094
1757567900177,1757567900182307,22464.504397815213,2512269948.1106014,114046.5,114046.6,18.549099585395307,114046.5,0.10599999763071537,nan,114048.2,2814289.6400000094
```
okm/depth1/BTC-USDT-SWAP.csv.xz
```
ets,lts,cvolume,camount,last,askprc,askvol,bidprc,bidvol,idxprc,markprc,oi
1757548800019,1757548800022208,0,0,113899.9,113900,8.995399798937141,113899.9,9.149099795501678,nan,113899.9,2814067.8900000113
1757548800029,1757548800032094,0,0,113899.9,113900,9.095399796701967,113899.9,8.879099801536649,nan,113899.9,2814067.8900000113
1757548800049,1757548800052447,0,0,113899.9,113900,9.095399796701967,113899.9,9.138099795747548,nan,113899.9,2814067.8900000113
1757548800059,1757548800062394,0,0,113899.9,113900,9.32649979153648,113899.9,9.733599782437086,nan,113899.9,2814067.8900000113
1757548800069,1757548800072347,0,0,113899.9,113900,9.32649979153648,113899.9,9.509799787439405,nan,113899.9,2814067.8900000113
1757548800079,1757548800082285,0.019999999552965164,2277.999949082732,113900,113900,9.306499791983514,113899.9,9.600599785409868,nan,113899.9,2814067.8900000113
1757548800089,1757548800092255,0.02009999955072999,2289.389938828146,113899.9,113900,9.306499791983514,113899.9,9.549499786552042,nan,113899.9,2814067.8900000113
1757548800109,1757548800112586,0.02009999955072999,2289.389938828146,113899.9,113900,9.436099789086729,113899.9,8.094099819082766,nan,113899.9,2814067.8900000113
1757548800119,1757548800122581,0.02009999955072999,2289.389938828146,113899.9,113900,9.692299783360212,113899.9,8.030599820502102,nan,113899.9,2814067.8900000113
1757548800129,1757548800132383,0.02009999955072999,2289.389938828146,113899.9,113900,9.692299783360212,113899.9,7.960399822071194,nan,113899.9,2814067.8900000113
1757548800139,1757548800142463,0.02009999955072999,2289.389938828146,113899.9,113900,9.692299783360212,113899.9,7.949399822317065,nan,113899.9,2814067.8900000113

...

1757567899557,
1757567899559865,
8512.393409725695,
970626819.9734449,
114046.5,
114046.6,
18.546399585455656,
114046.5,
0.14999999664723873,
nan,
114048.4,
2814289.6400000094

1757567899737,1757567899739759,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.1498999966494739,nan,114048.4,2814289.6400000094
1757567899917,1757567899919927,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.10609999762848019,nan,114048.3,2814289.6400000094
1757567899927,1757567899929858,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.14969999665394426,nan,114048.3,2814289.6400000094
1757567899937,1757567899939934,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.1498999966494739,nan,114048.3,2814289.6400000094
1757567900087,1757567900089964,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.10609999762848019,nan,114048.3,2814289.6400000094
1757567900127,1757567900129723,8512.393509725693,970626831.3780947,114046.5,114046.6,18.546399585455656,114046.5,0.10599999763071537,nan,114048.2,2814289.6400000094
1757567900177,1757567900179874,8512.393509725693,970626831.3780947,114046.5,114046.6,18.549099585395307,114046.5,0.10599999763071537,nan,114048.2,2814289.6400000094
1757567900247,1757567900249848,8512.499809723315,970638954.5207136,114046.2,114046.2,18.097199595496058,114046.1,0.14819999668747186,nan,114048.2,2814289.6400000094
```


oks/depth1/ETH-USDT-SWAP.csv.xz
```
ets,lts,cvolume,camount,last,askprc,askvol,bidprc,bidvol,idxprc,markprc,oi
1757567899554,1757567899559221,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09000156596304,nan,4406.22,7614370.990000015
1757567899654,1757567899661339,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09100156597793,nan,4406.2,7614370.990000015
1757567899674,1757567899679438,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09000156596304,nan,4406.2,7614370.990000015
1757567899684,1757567899689362,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09100156597793,nan,4406.2,7614370.990000015
1757567899734,1757567899739382,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09200156599285,nan,4406.2,7614370.990000015
1757567899754,1757567899760728,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09300156600774,nan,4406.2,7614370.990000015
1757567899764,1757567899769438,529.2560078865371,2332959.949133825,4406.38,4406.39,110.4010016451031,4406.38,105.09100156597793,nan,4406.2,7614370.990000015
1757567899774,1757567899781703,531.1640079149685,2341367.341379105,4406.39,4406.39,110.4010016451031,4406.38,105.09300156600774,nan,4406.2,7614370.990000015
1757567899784,1757567899791850,531.8950079258613,2344588.4052071027,4406.38,4406.39,108.49300161667169,4406.38,104.36200155511497,nan,4406.2,7614370.990000015
1757567899794,1757567899799892,531.8950079258613,2344588.4052071027,4406.38,4406.39,108.49300161667169,4406.38,104.35900155507028,nan,4406.2,7614370.990000015
1757567899804,1757567899825773,531.8950079258613,2344588.4052071027,4406.38,4406.39,108.49300161667169,4406.38,104.36200155511497,nan,4406.2,7614370.990000015
1757567899824,1757567899832815,531.8950079258613,2344588.4052071027,4406.38,4406.39,108.49300161667169,4406.38,104.3640015551448,nan,4406.21,7614370.990000015
```

okm/depth1/ETH-USDT-SWAP.csv.xz
```
ets,lts,cvolume,camount,last,askprc,askvol,bidprc,bidvol,idxprc,markprc,oi
1757548800012,1757548800015469,0,0,4347.99,4348,115.36400171905757,4347.99,91.07400135710836,nan,4347.59,7378407.38000002
1757548800022,1757548800025344,0,0,4347.99,4348,115.36400171905757,4347.99,93.45400139257312,nan,4347.65,7378407.38000002
1757548800112,1757548800115622,0,0,4347.99,4348,115.36400171905757,4347.99,89.55300133444369,nan,4347.65,7378407.38000002
1757548800122,1757548800125275,0.0020000000298023225,8.6959801295802,4347.99,4348,115.36600171908736,4347.99,88.27900131545961,nan,4347.65,7378407.38000002
1757548800132,1757548800135359,0.0020000000298023225,8.6959801295802,4347.99,4348,115.42600171998143,4347.99,83.22000124007464,nan,4347.65,7378407.38000002
1757548800142,1757548800145458,0.0020000000298023225,8.6959801295802,4347.99,4348,132.42600197330117,4347.99,75.92000113129616,nan,4347.65,7378407.38000002
1757548800162,1757548800165188,0.0020000000298023225,8.6959801295802,4347.99,4348,132.42600197330117,4347.99,74.12000110447407,nan,4347.65,7378407.38000002
1757548800172,1757548800175478,0.0020000000298023225,8.6959801295802,4347.99,4348,136.47900203369556,4347.99,26.93000040128827,nan,4347.65,7378407.38000002
1757548800182,1757548800185357,0.0020000000298023225,8.6959801295802,4347.99,4348,119.47900178037584,4347.99,22.764000339210032,nan,4347.65,7378407.38000002
1757548800192,1757548800195250,0.0020000000298023225,8.6959801295802,4347.99,4348,119.47900178037584,4347.99,19.423000289425254,nan,4347.65,7378407.38000002
1757548800222,1757548800225573,0.0020000000298023225,8.6959801295802,4347.99,4348,119.47900178037584,4347.99,10.937000162974,nan,4347.65,7378407.38000002
```
# day008 - 20250917
# day009 - 20250918
# day010 - 20250919


## 时间相关
这段代码用于计算金融市场中的关键时间点（以纳秒为单位的时间戳），基于给定的日期字符串（`date`）和交易所标识（`exchange`）。以下是逐行解释：

### 1. **市场开盘时间（`mktopen`）**
   ```cpp
   const long mktopen = stol(date) * 1'000'000'000 + 9'00'00'000;
   ```
   - **功能**：计算当日市场开盘时间。
   - **逻辑**：
     - `stol(date)`：将字符串格式的日期（如 `"20230919"`）转换为整数（`20230919`）。
     - `* 1'000'000'000`：将日期整数转换为纳秒单位（1秒 = 10^9纳秒），表示当日 00:00:00 的时间戳。
     - `+ 9'00'00'000`：加上 9 小时（9:00:00）的纳秒数，得到 **当日 09:00:00** 的时间戳。
   - **结果**：`mktopen` 代表开盘时间（如 `20230919090000000000`）。

---

### 2. **市场收盘时间（`eod`）**
   ```cpp
   const long eod = exchange == "CFFEX"? 
                   stol(date) * 1'000'000'000 + 15'15'00'000 : 
                   stol(date) * 1'000'000'000 + 15'00'00'000;
   ```
   - **功能**：根据交易所类型计算当日收盘时间。
   - **逻辑**：
     - **若交易所是 `CFFEX`（中国金融期货交易所）**：
       - 收盘时间为 **15:15:00**（下午 3:15）。
       - 时间戳 = 当日 00:00:00 + 15小时15分钟的纳秒数。
     - **其他交易所**：
       - 收盘时间为 **15:00:00**（下午 3:00）。
       - 时间戳 = 当日 00:00:00 + 15小时的纳秒数。
   - **结果**：`eod` 代表收盘时间（如 `20230919150000000000`）。

---

### 3. **夜间时段开始时间（`son`）**
   ```cpp
   const long son = stol(date) * 1'000'000'000 + 21'00'00'000;
   ```
   - **功能**：计算当日夜间交易时段的开始时间（常见于期货市场）。
   - **逻辑**：
     - `stol(date) * 1'000'000'000`：当日 00:00:00 的时间戳。
     - `+ 21'00'00'000`：加上 21 小时（21:00:00）的纳秒数，得到 **当日 21:00:00** 的时间戳。
   - **结果**：`son` 代表夜盘开盘时间（如 `20230919210000000000`）。

---

### **关键细节**
- **时间单位**：所有时间戳以纳秒（10^-9秒）为单位，适合高精度时间计算。
- **日期格式**：`date` 应为 `YYYYMMDD` 格式的字符串（如 `"20230919"`）。
- **交易所差异**：仅 `CFFEX` 的收盘时间比其他交易所晚 15 分钟（15:15 vs 15:00）。
- **代码语法**：`1'000'000'000` 中的单引号是 C++14 的数字分隔符，提高可读性（等价于 `1000000000`）。

---

### **典型应用场景**
此类代码常见于金融交易系统，用于：
1. 判断交易时段（如开盘至收盘为日盘，21:00 至次日为夜盘）。
2. 触发定时操作（如收盘时自动平仓）。
3. 过滤非交易时段的数据。

**示例**：  
若 `date = "20230919"`, `exchange = "SHFE"`（非 CFFEX）：
- `mktopen` = 20230919 09:00:00（纳秒时间戳）  
- `eod` = 20230919 15:00:00  
- `son` = 20230919 21:00:00

## 文件处理相关
temp_path 指示了一个 临时文件的 目录。`temp_directory_path() / tempfile_XXXXXX`。

fdata目录结构：
1. raw
    1. tick
        1. CFFEX
        2. CTP
        3. CZCE
        4. DCE
        5. GFEX
        6. INE
        7. MDC
        8. SHFE
        9. TAP
            1. 这些文件夹下都存在两个文件夹：`depth1`、`depth5`
                1. 下面就是 20250709 这样的各种日期
                    1. 进去之后，就是 futopt + 各种 future （如JD2509）的`csv.xz`。
                        1. future 字段：ets,lts,cvolume,camount,oi,last,askprc,askvol,bidprc,bidvol,avgaskprc,totaskvol,avgbidprc,totbidvol
                        2. futopt 字段：ets,lts,**ticker**,cvolume,...

`cmp_xz.cpp`的工作：
1. 第一个参数是 交易所 的标识符，如`DCE`
2. 第二个参数是 日期 

`tstdir` 的 目录是 `/home/fdata/raw/tick/参数1/depth1/参数2`
`bendir` 的 目录是 `/home/fdata/raw/tick/MDC/depth1/参数2`

接下来，把 tstdir 的 文件夹 下的 文件遍历
```cpp
set<string> ListFilesUnderFolder(const string& dir, bool fullpath)
{
    set<string> filenames;
    for (const auto& p : std::filesystem::directory_iterator(dir))
    {
        if (fullpath)
            filenames.insert(p.path());
        else
            filenames.insert(p.path().filename());
    }
    return filenames;
}
```

也就是，遍历 一个日期的 futopt + 各种 future （如JD2509）的`csv.xz`。

代码逻辑中，排除了 futopt 文件。排除了 exchange（参数1）为 SHFE 且 底下的 csv.xz 文件 为 `BC`、`EC`、`LU`、`NR`、`SC`开头的。

之后：

tstpath 为 `tstdir` + `/` + `f`，即 `/home/fdata/raw/tick/参数1(exchange)/depth1/参数2(date)/?.csv.xz`

benpath 为 `bendir` + `/` + `f`，即 `/home/fdata/raw/tick/MDC/depth1/参数2(date)/?.csv.xz`

创建几个文件流：
1. tst 的输入文件流 tfs，二进制模式打开，文件路径是 tstpath。用来读取文件的内容。
2. tst 的过滤输入流 tin。这种流允许将多个过滤器（如解压缩器）串联起来，数据在读取过程中会经过这些过滤器的处理。
    1. `tin.push(boost::iostream::lzma_decompressor())`。向过滤流 tin 中推送一个 LZMA 解压缩器过滤器。这意味着，之后从tin读取数据时，数据会自动被 LZMA 算法解压。
    2. `tin.push(tfs)`。将之前打开的原始文件流 tfs 推送（连接）到过滤流 tin 的末端。作为整个过滤器链的数据链。
    3. 至此，数据流的方向是：从 tfs （磁盘上的压缩文件）读取 `->` 经过 `lzma_decompressor` 解压 `->` 从 tin 接口 输出解压后的数据。
3. tst 的输出文件流 tof。用于写入一个文件。文件路径是`tempfile`这个变量决定的。这个文件将用于保存解压后的数据。

`boost::iostream::copy(tin, tof)`。Boost Iostreams 的 copy 函数，将从过滤输入流 tin 中读取的所有数据（已经经过 LZMA 的解压），复制并写入到输出文件流 tof 中。

之后，`tos::close()`，确保所有解压后的数据都已正确写入磁盘文件 tempfile 中，并释放相关资源。

现在，tempfile 已经有解压后的 csv 数据了。

调用`csv::CSVReader{tempfile}` 结合 for 循环 就可以 每次取出一行，存到 row 中。
通过`row.["ets"].get<long>`就能取出 row 中的某个字段。填到 MarketDataDepth1 md。

预申请的对象：`map<long, MarketDataDepth1> tmd, bmd;`

最后，把组合好的 md 填写到 `tmd[md.ets]`。

这是对 tst 的读入处理。

对 ben 的处理也是一样，先创建几个文件流，最后 for 循环 写入 bmd。

tmd、bmd 都写好了以后：

对 tmd 中 每个 key - value：（范围 for）
如果`ets < mktopen`，跳过。如果 ets 大于 eod 并且 小于 son，跳过。
其他为正常情况：
把 tmd 的 ets 和 bmd 中 对比，如果 bmd 不 contains 这个 ets，则 输出：哪个文件下，tmd 中 出现了 额外的 ets（意思就是，bmd 中没有）。然后，跳过。

如果 bmd 存在 这个 ets，取出 bmd 中 这个 ets 对应的 value（一个MarketData）。去判断，两个MarketData 是否 一样？如果不一样，则打出哪个文件下的 ets 出现了`inconsistent`。

对 bmd 中 每个 key-value：（范围 for）
和 上面 tmd 的逻辑 稍有不同，
先判断 tmd 是否 不包含 bmd 的 每个 key。
如果不包含，才去执行下面的：

如果`ets < mktopen`，跳过。如果 ets 大于 eod 并且 小于 son，跳过。
其他情况：（交易时段的 `ets` ，并且其在 `tmd`中缺失，则尝试寻找最接近的（更早的）时间戳进行比较）

取 tmd 对 ets 的 `lower_bound(ets)`，（在 tmd 中查找**第一个不小于** ets 的时间戳）称为 ltick。（注意，`lower_bound`取的是迭代器）

> 因此 lower_bound 可以 理解为，按一个 key 值（这个 key 可以不属于此容器）寻找**下**边界，这个**下边界** 有可能 是自己（因为不小于包含了等于、大于的情况）。确定了下界。lower 这个名字容易产生误解，不是 前一条 key、小一条 key。
> 还需要注意的是`upper_bound`，寻找的是 第一个 大于 某值 的 key。确定了上界。upper 这个名字容易产生误解，不是 上一条 key。
> 例子：以序列 `{1, 2, 2, 4, 5}`，查找值 2 为例，`lower_bound`指向​**第一个** 2 的迭代器 (即第一个 `>=2`的元素)，而 upper_bound 指向了 4 ，这样就直观地理解了！对应了 Cpp中 左闭右开的规则。

如果 tmd 中 对于 ets（来自bmd） 的 **第一个不小于** ets（来自bmd）的时间戳 是 自己 最小的 ets。就说明 在 tmd 中 没有 这个 bmd 中 ets。打印 missing。

>比如：bmd 中 出现了 10 这个 ets，但它在 tmd 中不存在。比如 tmd 的 begin key 是 12。（肯定是大于 10 的数，因为前提条件是 tmd 中不存在 10），tmd 中 第一个不小于 10 的 ets 应该 是 12 （begin）。所以，可以看作 tmd miss。



如果不是自己的 开始边界。`--ltick`，去找上一条。此时，如果 `ltick` 的 value 不等于 bmd 的 `md` 。也视为缺 bmd 中的 这个 ets。

>比如：bmd 中 出现了 10 这个 ets，但它在 tmd 中不存在。比如 tmd 的 begin key 是 8。tmd 中 第一个不小于 10 的 ets 应该 不是 8 （begin）。如果 tmd 存在 12，则 ltick 指向的是 12 的迭代器。此时，让 `ltick` 去找 tmd 中的上一个数（ `--`），应该是一个小于 10 的数，此例中为 8。去看 tmd 中 8 这个 ets 对应的 data 是否 和 bmd 中一样，如果不一样，则可以看作 tmd 有 miss！即正常。

# day011 - 20250922
## 分析出的问题
1. 20250603 的 DCE depth5，20250603101500025（79个文件）、20250603113000048（99个文件）。出现 1 次重复的 ets，导致数据不一致。
2. 20250605 的 DCE depth5，20250605230000008（49 个文件）。出现 1 次重复的 ets，导致数据不一致。
3. 20250606 的 DCE depth5，20250606101500015（67个文件）。出现 1 次重复的 ets，导致数据不一致。
4. 20250611 的 DCE depth5，20250611113000019（125 个文件）。出现 1 次重复的 ets，导致数据不一致。
5. 20250612 的 DCE depth5，20250612101500026（64 个文件）、20250612113000044（110 个文件）、20250612230000012（56 个文件）。出现 1 次重复的 ets，导致数据不一致。
6. 20250613，20250613101500015（80 个）。出现 1 次重复的 ets，导致数据不一致。
7. 20250616，20250616150000022（88 个）。出现 1 次重复的 ets，导致数据不一致。
8. 20250617，20250617230000034（60 个）。同上。
9. 20250618，20250618113000033。同上。
10. 20250619，同上
11. 20250620，同上
12. 20250626，同上
13. 20250702，同上
14. 20250703，同上
15. 20250704，同上
16. 20250708，同上
17. 20250710，同上
18. 20250714，同上
19. 20250717，同上
20. 20250718，同上
21. 20250725，同上
22. 20250728，同上
23. 20250730，同上
24. 20250731，同上
25. 20250804，同上
26. 20250805，同上
27. 20250806，同上
28. 20250807，同上
29. 20250812，同上
30. 20250813，同上
31. 20250818，同上
32. 20250820，同上
33. 20250821，同上
34. 20250822，同上
35. 20250825，同上
36. 20250826，同上
37. 20250828，同上
38. 20250829，同上
39. 20250901，同上
40. 20250902，同上
41. 20250908，同上
42. 20250910，同上
43. 20250911，同上
44. 20250912，同上
45. 20250917，同上
46. 20250918，同上
47. 20250919。
    1. **Missing ETS：1245372 个。全部都是因为 DCE 21:00 - 23:00 没有 相应的 ets 记录**。
    2. 数据不一致：114 个 文件，20250919113000029 时 出现 ets 重复 1 次，导致数据不一致。
# day012 - 20250923
# day013 - 20250924
# day014 - 20250925
# day015 - 20250926
ToDo:
1. 调研 虚拟局域网 安全性
2. 调研 flat map。
3. 每天定时比对 project。包括 CZCE 都比。
    1. 调研 mmap ，提出 CSV 库 issue，或解决方案。
    2. 调研 csv 流式解析 bug https://github.com/vincentlaucsb/csv-parser/issues/272
4. 写 BINANCE 的 depth1 对比 算法。（相同ets，按 lts 条数 或 时间顺序 比对 数据）
5. 看期货、期权书籍
6. 调研 infinity ，基于 C++20 modules 的 代码规范不错的 开源库（可选
7. 调研 quill issue https://github.com/odygrd/quill/issues/830
# day016 - 20250928
# day017 - 20250929
http://103.91.178.50:8888/hnie/nutils.git
nutils之后会作为一个公司长期维护的仓库，大家可以把写的比较精简的代码上传
# day018 - 20250930
# day019 - 20251009
用现有第三方CSV库的问题及优化方案：
1. A 方案：用官方示例给出的流式内存解析（`CSVReader(istream, fmt)`）很方便，但是有时会报 ets not a number 错误。现已基本确定是库的bug。 https://github.com/vincentlaucsb/csv-parser/issues/272
    1. 本质不是 ets 不是数字，而是整行被解析成了单个字段，`row["ets"] `里塞进了“半截上一行 + 换行 + 下一行整行”的串。这通常由脏控制字符或引号状态导致跨行；
    2. 已经证实：已清洁的原数据（比如20250821 MDC depth5 TA2601 的第 61640 行）是好的，但库在流式解析中还是给了粘行 → 说明是库的流式路径在某些边界条件下会失准（比如缓冲区切片边界、控制字符、行超长等），而不是原文的那一行坏。
2. B 方案：可以保存到临时文件解析进行缓解（`CSVReader{tempfile}`），CSV库底层的实现是用多线程 + mmap的方式对文件操作的。但会出现`MmapParser::next()` 抛 `std::error_code` 的崩溃现象。目前还未排查出根本问题，仍在调研，基本确定库的 mmap/线程实现耦合、边界处理时产生bug有关。

目前的方案：基于 A 方案（流式内存解析）细致地优化，避免bug。
1. 清理干扰字符：`sanitize_csv_inplace`：对 行尾、引号`"`、除了`\n`以外的控制字符（0x00 ～ 0x1f）清理。
2. 行视图：`split_lines_views` 把“清洁后的整段文本”按 `\n` 切成 `string_view` 数组 `lines`；
3. csv-parser 配置（方言声明）：
    1. CSVFormat fmt：
        1. `.delimiter(',')`、`.trim({' ','\t','\r'})`、`.header_row(0)`、`.variable_columns(IGNORE_ROW)`
        2. `.quote('\0')`（等价于 `no_quote(true)`）：彻底关闭引号解析，避免“半个引号让状态机跨行”。
4. 解析文件流程（新增兜底修复feature）
    1. 正常解析：
        1. 用 `row["ets"].get_sv() + from_chars` 解析整型时间戳；
        2. 价格/量用 `parse_double_sv`（自带 trim + nan 处理）；
        3. 列取法是固定列位（`ap/av/bp/bv` 各 5 列）。
    2. 单元格级修复（最关键，解决“粘行”）
        1. 触发条件：`row.size() != expected_cols` 或 ets 解析失败。
        2. `recover_all_segments_into_map(...)`：
            1. 把 `row[0]` 单元格`(ets)`当文本看，按 `\n` 切成多个片段；
            2. 对每个片段用 `parse_fixed32_line` 尝试解析：凡是列数 `≥ 32` 的片段，截前 `32` 列按固定列位解析并写入 map；
            3. 注意是“多片段全部落表”，不是只恢复最后一个片段——这正好修掉“修复 N 行后，`N+1` 行 `missing`”的问题（`N`、`N+1` 都是同一次迭代里的片段，我们全吃下）。
        3. 日志形如 `[FIX*][TEST] FG2601.csv.xz : line 57533 recovered 2 segment(s) ...`，能看见一次迭代回收了几条物理行。
    3. 物理行兜底（完全跳过 csv-parser）
        1. 如果单元格修复也没产出任何行，就用 `lines[line_number]`（已清洁原文的对应物理行）调用 `parse_fixed32_line` 直接解析；
        2. 成功就 `[FALLBACK][TEST] ... parsed from raw line`。
        3. 这保证“哪怕 csv-parser 在某行完全失准，也不丢数据”。
5. 收尾回填（Backfill，全表扫描补洞）
    1. `backfill_from_raw_lines(...)` 在循环结束后跑一次：对 `lines[1..]` 逐行按 32 列解析；
    2. 如果该行的 ets 不在 map 里，就补入；
    3. 这是最终保险，确保“某次迭代吃进多行但仍有片段遗漏”也能被补上，从而彻底消灭“修复 N 行后 N+1 行 missing”的误报。
## 相比原程序，关键改动点
1. **解析路径切换：文件路径 → 内存流**
   * 原来：`CSVReader{tempfile}`（传文件路径）→ 触发库的 **MmapParser + 后台线程**。
   * 现在：`.xz → string → istringstream → CSVReader(istream, fmt)`，彻底绕开 mmap/多线程读。
   * 作用：避免了 `MmapParser::next()` 抛 `std::error_code` 的崩溃栈。

2. **预处理更“狠”**

   * 去 `\r`、去所有 `"`、**去除除 `\n` 外的所有控制字符 (0x00–0x1F)**。
   * 作用：避免脏字符把解析状态机搞乱（行被“吞并”、列数跑飞）。

3. **明确 CSV 方言，禁用引号**

   * `fmt.delimiter(',').trim({' ','\t','\r'}).header_row(0).variable_columns(IGNORE_ROW)`
   * **`fmt.quote('\0')`**（若版本支持就用 `no_quote(true)`）。
   * 作用：目前的 csv 文件都没有引号，关掉引号解析能避免“半个引号导致跨行”的经典坑。

4. **更稳的取值**

   * `row["ets"].get_sv()` + `from_chars` 做整数解析；浮点用自写 `trim + stod`。
       * todo：浮点能不能用from_chars，统一代码风格。
   * 作用：避免库在数值转换上的偶发问题（比如空白、不可见字符）。

5. **两级兜底，最关键的“救命绳”**
   * **A. 单元格修复（ `[FIX ] recovered from singleton cell`）**
     * 只要发现列数异常或 `ets` 解析失败，就把 `row[0]` 里被粘进去的**多行**按 `\n` 切开，**逐段尝试**；一旦某段 **≥ 32 列**，就**截前 32 列**按固定列位解析。
     * 这一步是这次最管用的！直接把“整行被吞进一个字段”的情况救回来了。
   * **B. 行级兜底（FALLBACK）**
     * 如果 A 还救不回，就从**已清洁的原文文本**里按**物理行号**取该行，手工按逗号切 32 列解析。
     * 作用：就算库的迭代器在某行完全失准，也不丢数据。

6. **健壮性监控**
   * 打印 **构建指纹**（确认跑的是新二进制）、`[HDR]` 列名、`[DBG]` 指定行内容；
   * 对异常行打印 `[WARN]`，对被修复/兜底的行打印 `[FIX]/[FALLBACK]`，便于量化问题规模。
## 原先程序为什么会挂 / 数据为什么会“丢”
* **崩溃**：你最初是 `CSVReader{tempfile}` → 走 mmap + 线程，栈里就是 `MmapParser::next()` 抛 `std::error_code`。这一般与库的 mmap/线程实现耦合、边界处理有关（见你最早的 gdb）。
* **“not a number”**：本质不是 `ets` 不是数字，而是**整行被解析成了单个字段**，`row["ets"]` 里塞进了“半截上一行 + 换行 + 下一行整行”的串。
    * 这通常由**脏控制字符**或**引号状态**导致跨行；你 bench 侧尤为明显。
    * 我们已经证实：**已清洁原文第 61640 行是好的**，但库在流式解析中还是给了粘行 → 说明是**库的流式路径在某些边界条件下会失准**（比如缓冲区切片边界、控制字符、行超长等），而不是原文的那一行坏。
## 归因：库 vs. CSV 文件？
* **两者叠加**，但主因更偏向**库的流式解析脆弱**：
    * 证据 1：已清洁文本行是正常的，而 `CSVReader(istream)` 仍能偶发给出“单字段粘行”，需要我们在 cell 内再拆 `\n` 才能恢复。
    * 证据 2：路径输入触发的 **MmapParser** 直接崩。我们改成内存流就稳定了。
* **数据也有影响**：如果原数据偶尔混入不可见控制字符、奇怪行尾，确实更容易触发库的边界 bug。我们“强力清洁”后，触发概率会显著下降。
## 现在这套方案的取舍
* **优点**：
    * 不改库源码、改动小；
    * 既绕开 mmap/线程，又对流式解析的粘行提供“单元格修复 + 行级兜底”的双保险；
    * 生产可用，遇到极少数坏行也不会整批失败。
* **代价**：
    * 代码里多了一点“解析恢复”的逻辑；
    * 日志会记录 `[FIX]/[FALLBACK]`，但这是好事，能量化源数据/库的问题规模。
## 给生产的建议

1. 保持现在的 **A 方案**（内存解压 + 强清洁 + 禁用引号 + `istream` + 双兜底）。
2. 监控：统计 `[FIX]`/`[FALLBACK]` 的行数占比；明显上升时，优先检查数据源是否混入非打印字符。
3. 若后续升级 `csv-parser`（或改用 Arrow/fast-cpp-csv-parser 等），仍建议保留**清洁**和**列数自检**这两道基本防线。
4. 如果你确认文件行一定固定 32 列且无引号，将来完全可以换成**自写轻量 parser**（我们已经给过原型），性能更可控、行为可证明。

一句话结论：

* 原程序的问题是**使用了库的 mmap/线程解析路径 + 对脏字符/跨行缺乏防护**；
* 我们的改动绕开了易出错路径，并为**粘行**提供了两级恢复，所以你看到“recovered from singleton cell”后，早盘缺失也消失了；
* **既有少量数据质量噪声，也有库流式路径的脆弱点**，这次的修正同时对两者兜底。


CSV第三方库流式解析问题。

https://github.com/vincentlaucsb/csv-parser/issues/272

有时会报"ets" not a number。（比如MDC下depth5下的20250821的TA2601，这是基准表格）

这是因为有粘行问题。

解决了粘行问题后，程序错误地报大量`missing ets(...) in test`。说明 test 侧在较早位置跳过了不少行（通常是个别行ets解析失败）。
# day020 - 20251010
调研爬虫



```python
from seleniumbase import SB
from selenium.webdriver.common.by import By
import sys, time, random


def r_sleep(
    base_seconds: float,
    jitter: float = 0.5,
    min_sec: float = 0.3,
    max_sec: float = None,
):
    """Sleep for a randomized duration around base_seconds.
    Args:
        base_seconds: The central duration in seconds.
        jitter: Fractional +/- range to randomize (0.5 => between 50% and 150% of base).
        min_sec: Minimum cap.
        max_sec: Optional maximum cap.
    """
    low = max(min_sec, base_seconds * (1 - jitter))
    high = base_seconds * (1 + jitter)
    if max_sec is not None:
        high = min(high, max_sec)
    duration = random.uniform(low, high)
    time.sleep(duration)


class CrawlerDCE:
    def __init__(self, sb):
        self.sb = sb

    def __getattr__(self, name):
        return getattr(self.sb, name)

    def download_settlement_file(self, target_date: str):
        print(">> download ", target_date)
        self.open("http://www.dce.com.cn/dce/channel/list/181.html")
        if not self.headless:
            self.maximize_window()
        self.wait_for_element_visible("#variety-iframe", timeout=30)
        iframe = self.find_element(By.ID, "variety-iframe")
        if iframe.is_displayed():
            r_sleep(1)
            js = r"""
            const iframe = arguments[0];
            const value = arguments[1];
            const win = iframe.contentWindow;
            const doc = iframe.contentDocument || win.document;
            
            const input = doc.querySelector('.ant-picker-input input');
            if (!input) { return 'no-input'; }
            
            // remove readonly attribute if present
            input.removeAttribute('readonly');
            input.removeAttribute('disabled');
            
            // focus and simulate keyboard input
            input.focus();
            input.click(); // Ensure focus
            
            // select all and delete existing content
            input.select();
            for (let char of value) {
                input.dispatchEvent(new KeyboardEvent('keydown', { key: 'Backspace', bubbles: true }));
                input.dispatchEvent(new KeyboardEvent('keydown', { key: 'Delete', bubbles: true }));
            }
            // type each character of the date
            for (let char of value) {
                input.dispatchEvent(new KeyboardEvent('keydown', { key: char, bubbles: true }));
                input.dispatchEvent(new KeyboardEvent('keypress', { key: char, bubbles: true }));
                input.dispatchEvent(new KeyboardEvent('keyup', { key: char, bubbles: true }));
                // Also try setting value incrementally
                input.value += char;
                input.dispatchEvent(new Event('input', { bubbles: true }));
            }
            
            // sleep for a moment after setting value
            setTimeout(() => {
                // fire comprehensive event sequence
                input.dispatchEvent(new Event('focus', { bubbles: true }));
                input.dispatchEvent(new Event('input', { bubbles: true }));
                input.dispatchEvent(new Event('change', { bubbles: true }));
                input.dispatchEvent(new Event('blur', { bubbles: true }));
                
                // wait a moment for calendar to appear, then click the target date cell
                setTimeout(() => {
                    console.log('Looking for date cell with value:', value);
                    
                    // try to find and click the date cell for the specific date
                    const dateCell = doc.querySelector(`td[title="${value}"]`);
                    console.log('Found dateCell:', dateCell);
                    if (dateCell) {
                        dateCell.click(); // Use regular click instead of dispatchEvent
                        console.log('Clicked date cell with title:', value);
                        // Press Enter to confirm selection
                        input.dispatchEvent(new KeyboardEvent('keydown', { key: 'Enter', bubbles: true }));
                        return 'clicked-date-cell';
                    }
                    
                    // fallback: try clicking any cell with text matching the day
                    const dayText = value.split('-')[2]; // Get "08" from "2025-08-08"
                    const dayNumber = parseInt(dayText, 10).toString(); // "08" -> "8"
                    console.log('Looking for day number:', dayNumber);
                    
                    const cells = doc.querySelectorAll('.ant-picker-cell .ant-picker-cell-inner');
                    console.log('Found cells:', cells.length);
                    for (let cell of cells) {
                        if (cell.textContent.trim() === dayNumber) {
                            cell.click(); // Use regular click
                            console.log('Clicked day cell:', dayNumber);
                            // Press Enter to confirm selection
                            input.dispatchEvent(new KeyboardEvent('keydown', { key: 'Enter', bubbles: true }));
                            return 'clicked-day-' + dayNumber;
                        }
                    }
                    
                    // last resort: try OK button
                    const okBtn = doc.querySelector('.ant-picker-ok .ant-btn, .ant-picker-footer button');
                    if (okBtn) {
                        okBtn.dispatchEvent(new MouseEvent('click', { view: win, bubbles: true }));
                        return 'clicked-ok-button';
                    }
                    
                    return 'no-clickable-found';
                }, 1000);
            }, 1000);
            
            return 'date-set-waiting-for-click';
            """
            result = self.execute_script(js, iframe, target_date)
            print("JS result:", result)

            # wait longer for the setTimeout callbacks to execute
            print("waiting for calendar interaction...")
            r_sleep(3, min_sec=2)

            # verify what actually happened
            verify_js = r"""
            const iframe = arguments[0];
            const value = arguments[1];
            const doc = iframe.contentDocument || iframe.contentWindow.document;
            
            // check input field value
            const input = doc.querySelector('.ant-picker-input input');
            const inputValue = input ? input.value : 'no-input';
            
            // check if calendar is visible
            const calendar = doc.querySelector('.ant-picker-panel, .ant-picker-dropdown');
            const calendarVisible = calendar && calendar.offsetParent !== null;
            
            // check if day exists and is selected 
            const dayCell = doc.querySelector(`td[title="${value}"]`);
            const dayExists = !!dayCell;
            const daySelected = dayCell && dayCell.classList.contains('ant-picker-cell-selected');
            
            // if calendar is still open, force click day
            if (calendarVisible && dayCell) {
                dayCell.click();
                return {
                    action: 'force-clicked-day',
                    inputValue: inputValue,
                    calendarVisible: true,
                    dayExists: dayExists,
                    daySelected: daySelected
                };
            }
            
            return {
                action: 'verification-complete',
                inputValue: inputValue,
                calendarVisible: calendarVisible,
                dayExists: dayExists,
                daySelected: daySelected
            };
            """

            verify_result = self.execute_script(
                verify_js, iframe, target_date.replace("\n", "")
            )
            print("verification result:", verify_result)

            # wait for data to refresh after date selection and calendar to fully close
            r_sleep(5, min_sec=3)

            # click the export button
            self.switch_to_frame(iframe)

            # Wait for any overlays to disappear and ensure export button is clickable
            self.wait_for_element_clickable(".export_btn", timeout=10)

            # Scroll to export button to ensure it's in view
            self.scroll_to_element(".export_btn")
            r_sleep(1)

            export_btn = self.find_element(By.CSS_SELECTOR, ".export_btn")
            export_btn.click()
            print("Export button clicked successfully")
            r_sleep(10, min_sec=5)  # Wait for download
            self.switch_to_parent_frame()
            print("END")
        else:
            print("iframe not displayed")


if __name__ == "__main__":
    with SB(uc=True, headless=False) as sb:
        crawler = CrawlerDCE(sb)
        # Convert YYYYMMDD to YYYY-MM-DD format
        date_input = sys.argv[1]
        if len(date_input) == 8 and date_input.isdigit():
            formatted_date = f"{date_input[:4]}-{date_input[4:6]}-{date_input[6:8]}"
        else:
            formatted_date = date_input
        crawler.download_settlement_file(f"{formatted_date}\n")

```
# day021 - 20251011
# day022 - 20251013
# day023 - 20251014
## 场景描述
节点的描述：是虚拟机？公网IPv4？动态IP？


场景描述：全球各地节点B。连接 国内节点A？A只能出站？

A能不能开放公网端口。
## 合规范围
合规性是否有要求？
是否要求全自建？
是否允许 第三方托管？（比如用CloudFlare）
## VPN传输数据界限
VPN用于什么数据传输？机机交互还是人机交互？
1. 人机访问：ssh，审计很重要

模块隔离？

1. 交易与行情。走专线。保证时延
2. 跨地研发、回测：自建专网


行情链路也需要专网控制吗？还是只是跑管理、运维、回测？

## 性能要求


性能要求？是否要求低开销、协议简单？

跨境网络质量是否敏感？


## mTLS - 应用层 - 不依赖固定IP白名单
mTLS（mutual TLS，双向 TLS）= **双方都用证书相互验证身份的 TLS**。  
普通 TLS 只验证“服务器是谁”；mTLS 在握手时还会验证“客户端是谁”。这样就不再依赖来源 IP、口令或可被窃取的 Token，而是**基于证书身份**放行。

## 有啥用（我们的场景）

- **只让自己机器访问 A**：把 A 的服务设为“必须持我司 CA 签发的客户端证书”才放行；陌生人即使撞上端口也进不来。
    
- **抗 IP 漂移/欺骗**：不再靠白名单 IP，机器换出口或被人伪造 IP 也没用。
    
- **零信任的一环**：配合专网/VPN（WireGuard/Tailscale/IPsec），做“网络层 + 身份层”双重门禁。
    
## 它怎么工作

1. 有一个**私有 CA**（公司根证书）。
2. 给 A 发 **服务器证书**；给每台能访问 A 的机器发 **客户端证书**（都由私有 CA 签）。
3. TLS 握手时：
    - A 出示服务器证书，客户端验证“这是 A（受我司 CA 信任）”。
    - 客户端也出示**客户端证书**，A 验证“你是受信的一员（并可校验角色/名称/SAN）”。
4. 双方验证通过后，建立加密通道并开始业务通信。
# day024 - 20251015
# day025 - 20251016
# day026 - 20251017
仿存性能：

1. 处理器从内存子系统中获取单字节的延迟
2. 每秒可以获取多少字节，即带宽。

x86平台，有用的工具之一是英特尔内存延迟检查器（MLC）。MLC 可以使用不同的访问模式和负载来测量缓存和内存的延迟和带宽。

如果是基于ARM平台，可以从下载一些源代码项目并构建内存延迟和带宽基准测试。这类的项目的示例包括`lmbench`、`bandwidth`、`Stream`。

# day027 - 20251020
数据规模20000个，但有效数据很稀疏，50个。存的只是bool真假值。

需要做 benchmark 调研：`std::vector<bool>`
、`boost::dynamic_bitset`、`std::bitset`、`flat_set<short>`、`flat_unordered_set<short>`的性能差异调研。

`std::bitset`是固定大小的位集。

`std::vector<bool>`是可扩展的位集。1 个 bool 数据占 1 位。

官方宣称，如果在编译时已知位集的大小，可以使用`std::bitset`，它提供了更丰富的成员函数集。

`boost::dynamic_bitset`可以作为`std::vector<bool>`的替代方案。

https://www.boost.org/doc/libs/latest/libs/dynamic_bitset/dynamic_bitset.html


还需要注意：

`std::vector<bool>`表示形式可能经过优化，`std::vector<bool>`不一定满足所有 Container 或 SequenceContainer 的要求。例如，由于 `std::vector<bool>` 是实现定义的，因此它可能不满足 LegacyForwardIterator 的要求。使用诸如 `std::search` 之类需要 LegacyForwardIterator 的算法可能会导致编译时或运行时错误 。

Boost.Container 版本的 vector 并不专门针对 bool 。

怎么做专业的benchmark调研，令人挑不出毛病、信服？

# day028 - 20251021
# day029 - 20251022
# day030 - 20251023
把bitset_bench的项目作为CMake Project推到了仓库。

构建过程：

```bash
git clone git clone git@<gitlab_ip>:cxing/qy-benchmark.git
```

如果终端没有公钥，需要生成一个：
```bash
ssh-keygen -t ed25519 -C "xing-cg@qq.com"
```

然后在gitlab账户配置中添加本机的SSH。`cat ~/.ssh/id_ed25519.pub`


然后，如果gitlab的SSH端口不是标准的22端口，则需要在终端的`~/.ssh/config`中配置host：
```bash
Host <gitlab_ip>
    HostName <gitlab_ip>
    Port <real_port>
    User cxing
    IdentityFile ~/.ssh/id_ed25519
```

之后便可以顺利地clone到本地了！


然后，切到benchmark项目根目录

便可以通过cmake一键配置、构建：
```bash
cmake . -B ./build
```

如果系统下的默认编译器不支持`C++23`的`<flat_set>`，则会提示：
```
(.venv) [cxing@turing001 qy-benchmark]$ cmake . -B ./build
-- The CXX compiler identification is GNU 11.5.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Performing Test HAS_STD_FLAT_SET
-- Performing Test HAS_STD_FLAT_SET - Failed
CMake Error at CMakeLists.txt:14 (message):
  当前编译器/标准库不支持<flat_set>。请重新配置:
  -DCMAKE_C_COMPILER=your_compiler -DCMAKE_CXX_COMPILER=your_compiler
```

那么，就需要在后面加额外编译指令：

```bash
cmake . -B ./build -DCMAKE_C_COMPILER=/opt/GCC/v15.2.0/bin/gcc-15 -DCMAKE_CXX_COMPILER=/opt/GCC/v15.2.0/bin/g++-15
```

便可以了。
但是，
这个项目的CMake配置过程，还需要git clone一下Google的Benchmark的repo，因此，如果终端的网络直连不了github，则会一直卡住。需要：`source ~/proxy.bashrc`

则就能下通了！


一切就绪后，就可以：`cmake --build ./build -j`

